<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TREC 2025: Million LLMs Track</title>
    <link rel="stylesheet" href="template.css">
</head>
<body>
    <figure style="margin: 0 auto; text-align: center; max-width: 800px;">
        <img src="img/MillionLLMs_vanGogh_style.webp" alt="Million LLMs Track Illustration" class="hero-image" />
        <figcaption style="font-size: 0.9rem; color: #666; margin-top: 0.5rem; font-style: italic;">
            Image generated by ChatGPT using the prompt: "Create an image in the style of Van Gogh depicting a million LLMs in humanoid form (as a metaphor), working together to respond to user queries."
        </figcaption>
    </figure>

    <h1>TREC 2025: Million LLMs Track</h1>
    <p class="tagline">Benchmarking the Discovery of Expert LLMs</p>
    <nav>
      <a href="#" title="Track GitHub">Track GitHub</a>
      <a href="#" title="Data Access">Data Access</a>
      <a href="https://app.slack.com/client/T43BZ4E0K/D08E5S14HSR" title="Slack">Slack</a>
      <a href="#" title="Evaluation Scripts">Evaluation Scripts</a>
    </nav>

     <div class="section" id="introduction">
      <h2>1. Introduction</h2>
      <p>The way we search for and access information is rapidly evolving. Instead of retrieving documents and snippets, 
        users now interact with AI front-ends—Large Language Models (LLMs) that deliver direct, synthesized answers. 
        In this emerging landscape, the traditional search engine is replaced by an ecosystem of LLMs, each with specialized 
        knowledge, unique capabilities, and distinct access models:
        <ul>
          <li>Some LLMs excel at in-depth answers; others provide concise, factual responses.</li>
          <li>Some are trained on general knowledge, while others focus on specific domains like law, medicine, or technology.</li>
          <li>Some LLMs are multilingual, while others specialize in a single language.</li>
          <li>Some LLMs are designed for specific tasks like summarization or translation, while others are general-purpose.</li>
          <li>Some LLMs are designed for specific user groups, such as children or professionals, while others are more general.</li>
        </ul>
      <p>How do we evaluate expertise, compare models, and build reliable multi-agent IR systems in this new world? </p>
      <p>The Million LLMs Track introduces a novel challenge: ranking large language models (LLMs) based on 
        their expected ability to answer specific user queries. As organizations deploy ensembles of LLMs—ranging from 
        general-purpose to domain-specific it becomes crucial to determine which models to consult for a given task. 
        This track focuses on evaluating systems that can effectively identify the most capable LLM(s) for a query, 
        without issuing new queries to the models.</p>
      <p>Participants are provided with LLM responses and metadata in advance, and must rank the LLMs for each test 
        query based solely on this information.</p>
    </div>

     <div class="section" id="task">
      <h2>2. Task</h2>
      <h3>LLM Ranking Task</h3>
      <p>Given a user query, and a set of LLM ids, your system must rank the LLM ids. The goal is to predict which LLMs 
        have the highest expertise on the query and are most likely to contribute in a high-quality answers.</p>
      <p>Your submission should consist of a ranked list of LLM ids for each query.</p>
    </div>

     <div class="section" id="data">
      <h2>3. Data Provided</h2>
      <p>The dataset is split into two parts:</p>
      <ul>
        <li>
          <strong>Discovery Set</strong>: Ranking LLM ids on the basis of their expertise is not possible if you do not know 
          anything about these LLMs. The discovery set provides a set of queries and precomputed responses from all LLMs, 
          along with metadata that can be used to predict the expertise of each LLM. 
          <ul>
            <li>A set of training queries</li>
            <li>Precomputed responses from all LLMs</li>
            <li>Response metadata</li>
          </ul>
        </li>
        <li>
          <strong>Development Set</strong>: This dataset is used to develop and evaluate your ranking system, and will imitate the
          final evaluation conditions, by NIST. It includes:
          <ul>
            <li>A set of development queries</li>
            <li>A qrel with an expertise label for each query-LLM pair</li>
          </ul>
        </li>
        <li>
          <strong>Test Set</strong>: This is a held-out set of queries for final evaluation. It includes:
          <ul>
            <li>A set of held-out queries</li>
          </ul>
        </li>
      </ul>
      <p>Use the discovery data to predict the expertise of each LLM and develop your ranking system, then submit your results on the test set.</p>
      <p><em>Note: No access to raw document collections or LLMs is required. The task is designed to benchmark ranking systems under fixed conditions using provided data.</em></p>
    
    <!-- Example: Add after the Data Provided section -->
  <div class="section" id="downloads">
  <h3>Downloadable Files</h3>
  <table>
    <thead>
      <tr>
        <th>File Name</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://drive.google.com/file/d/11haneITYfbZeDQ_Jwc_nIwJC5IfhbOJ4/view?usp=drive_link" target="_blank"><discovery_data_1 class="json">discovery_data_1</a></td>
        <td>LLM expertise discovery queries and precomputed LLM responses.</td>
      </tr>
      <tr>
        <td><a href="https://drive.google.com/file/d/1OASydzmu8WTQQwZusWgqkcqdkGwhDRyh/view?usp=drive_link" target="_blank"><discovery_metadata_1 class="json">discovery_metadata_1</a></td>
        <td>LLM expertise discovery queries, precomputed LLM responses, along with metadata (the same as discovery_data_1 but with logprobs).</td>
      </tr>
      <tr>
        <td><a href="https://drive.google.com/file/d/1fMJ4hvIdsYwbtuuSm8wBX91SBQw32gS3/view?usp=drive_link" target="_blank"><discovery_data_2 class="json">discovery_data_2</a></td>
        <td>LLM expertise discovery queries and precomputed LLM responses.</td>
      </tr>
      <tr>
        <td><a href="https://drive.google.com/file/d/1l4nEBTimaPUq9FtemH029L0J7hh4Tuv6/view?usp=drive_link" target="_blank"><discovery_metadata_2 class="json">discovery_metadata_2</a></td>
        <td>LLM expertise discovery queries, precomputed LLM responses, along with metadata (the same as discovery_data_2 but with logprobs).</td>
      </tr>
    </tbody>
  </table>
</div>
    </div>

     <div class="section" id="submission">
      <h2>4. Submission Guidelines</h2>

      <p>We will be following the classic TREC submission formatting, which is outlined below. White space is used to separate columns. 
        The width of the columns in the format is not important, but it is crucial to have exactly six columns per line with at least 
        one space between the columns.</p>
      <code>
        1 Q0 llmid1 1 2.73 runid1<br>
        1 Q0 llmid2 1 2.71 runid1<br>
        1 Q0 llmid3 1 2.61 runid1<br>
        1 Q0 llmid4 1 2.05 runid1<br>
        1 Q0 llmid5 1 1.89 runid1<br>
      </code>
      <p>Where:</p>
      <p>The first column is the topic (query) number.</p> 
      <p>The second column is currently unused and should always be "Q0".</p>
      <p>The third column is the official identifier of the ranked LLM.</p>
      <p>The fourth column is the rank at which the LLM is ranked.</p>
      <p>The fifth column shows the score (integer or floating point) that generated the ranking. This score must be in descending (non-increasing) order.</p>
      <p>The sixth column is the ID of the run being submitted.</p>

      <h3>Submission Types</h3>
      <p>The main type of TREC submission is <i>automatic</i>, which means that there is no manual intervention when running the 
        test queries. This means: You should not adjust your runs, rewrite the query, retrain your model, or make any manual 
        adjustments after seeing the test queries. Ideally, you should only check the test queries to verify that they ran 
        properly (i.e., no bugs) before submitting your automatic runs.</p> 
      <p>However, if you want to have a human in the loop for your run or make any manual adjustments to the model or ranking 
        after seeing the test queries, you can mark your run as <i>manual</i> and provide a description of the types of alterations 
        performed. </p>
      <p>Further, we expect that only the discovery data is used to assess the expertise of each LLM. If this is the case we 
        mark these submissions as <i>internal</i>. However, if you wish to use external tools, e.g. LLMs, search engines, data, 
        to verify the expertise of each LLM then you should mark your run as <i>external</i>.</p>
      
      <h3>Runs Allowed</h3>
      <p>Each team may submit up to <strong>5 official runs</strong>.</p>

      <h3>Submission Deadline</h3>
      <p><strong>September 2025</strong></p>
      
      <h3>Submission Portal</h3>
      <p><a href="#" title="Codalab submission">Codalab link TBD</a></p>
      
      <h3>System Description</h3>
      <p>You must provide a short description of your method, including any training data or models used.</p>
    </div>

     <div class="section" id="evaluation">
      <h2>5. Evaluation</h2>
      <p>Submissions will be evaluated using relevance-based metrics.</p>
      <h3>Evaluation Metrics</h3>
      <ul>
        <li><strong>nDCG@10</strong> – Discounted gain over relevance grades</li>
        <li><strong>MRR</strong> – Mean reciprocal rank</li>
      </ul>
      <p>Evaluation will be based on hidden ground-truth labels. Results will be reported in aggregate and per query category.</p>
    </div>

     <div class="section" id="timeline">
      <h2>6. Timeline</h2>
      <table>
        <thead>
          <tr>
            <th>Date</th>
            <th>Event</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>July 7, 2025</td>
            <td>Expertise discovery and development data released</td>
          </tr>
          <tr>
            <td>September 2025</td>
            <td>Test queries released</td>
          </tr>
          <tr>
            <td>September 2025</td>
            <td>Submission deadline</td>
          </tr>
          <tr>
            <td>October 2025</td>
            <td>Evaluation &amp; results shared</td>
          </tr>
          <tr>
            <td>November 2025</td>
            <td>TREC conference</td>
          </tr>
        </tbody>
      </table>
    </div>

     <div class="section" id="participation">
      <h2>7. Participation &amp; Contact</h2>
      <p>We welcome participation from:</p>
      <ul>
        <li>Academic researchers</li>
        <li>Industry teams</li>
        <li>Independent developers</li>
        <li>Open-source contributors</li>
      </ul>
      <!--
      <h3>Track Organizers</h3>
      <ul class="contact-list">
        <li><strong>Evangelos Kanoulas</strong> – University of Amsterdam</li>
        <li>✉️ Contact: <a href="mailto:organizer@example.com">organizer@example.com</a></li>
        <li>💬 Slack: <a href="#" title="Invite to Slack">Invite link</a></li>
        <li>🧵 Mailing List: <a href="#" title="Subscribe to mailing list">Subscribe here</a></li>
      </ul>
    -->
    </div>

    <div class="section">
        <h2>Organizers</h2>
        <table class="organizers-table">
            <tr>
                <td>
                    <img src="img/organizers/ekanoulas.jpeg" alt="Evangelos Kanoulas">
                    <div class="name">Evangelos Kanoulas</div>
                    <div class="affiliation">University of Amsterdam, The Netherlands</div>
                </td>
                <td>
                    <img src="img/organizers/peustratiadis.jpeg" alt="Panagiotis Eustratiadis">
                    <div class="name">Panagiotis Eustratiadis</div>
                    <div class="affiliation">University of Amsterdam, The Netherlands</div>
                </td>
                <td>
                    <img src="img/organizers/msanderson.jpeg" alt="Mark Sanderson">
                    <div class="name">Mark Sanderson</div>
                    <div class="affiliation">RMIT University, Australia</div>
                </td>
                <td>
                    <img src="img/organizers/jcallan.jpeg" alt="Jamie Callan">
                    <div class="name">Jamie Callan</div>
                    <div class="affiliation">Carnegie Mellon University, USA</div>
                </td>
            </tr>
        </table>
    </div>

    <div class="section">
        <h2>Co-Organizers</h2>
        <table class="organizers-table">
            <tr>
                <td>
                    <img src="img/organizers/yli.jpeg" alt="Yongkang Li">
                    <div class="name">Yongkang Li</div>
                    <div class="affiliation">University of Amsterdam, The Netherlands</div>
                </td>
                <td>
                    <img src="img/organizers/jqiao.jpeg" alt="Jingfen Qiao">
                    <div class="name">Jingfen Qiao</div>
                    <div class="affiliation">University of Amsterdam, The Netherlands</div>
                </td>
                <td>
                    <img src="img/organizers/vpal.jpeg" alt="Vaishali Pal">
                    <div class="name">Vaishali Pal</div>
                    <div class="affiliation">University of Amsterdam, The Netherlands</div>
                </td>
            </tr>
        </table>
    </div>

    <a href="#" class="back-to-top">↑</a>

    <script src="scripts.js"></script>
</body>
</html>
